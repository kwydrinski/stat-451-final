---
title: "Final"
output: html_notebook
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newpage
# 1) Color Pallets

When using color in visualizations there are different kinds of pallets that can
be used.

## a. Pallet 1

Describe the pallet and explain when you would use such a pallet.

```{r echo=FALSE, fig.align='center'}
knitr::include_graphics('1a.png')
```

***

The palette above is an example of sequential color schemes using multi-hued
colors. 

Each scheme is sequential because it allows the highlighting of order in data
through the use of shading. Within a given hue such as blue, there are multiple
shades from light to dark. The lightness or darkness of the hue can be used to
represent different levels of values. Typically, light hues represent low
values and dark hues represent high values.

Each scheme is multi-hued meaning the colors used are not just shades of a
single hue but instead, use multiple hues. When using multiple hues, the
palettes still provide a pleasing aethetic transition from lighter shades to
darker shades that preserves the implied sequential meaning of the collective
colors.

Sequential colors schemes are suited to highlighted data that can be categorized
into ordered categories. Examples could be age ranges, levels of experience,
density ranges, etc.  

One vizualiation that may beneit from this color scheme, is a tile plot. One of the main features of a heat map, is its use of color to distinguish ordinal categorical variables, in a spacial setting.  With a sequential color scheme as such, I would apply this theme to a heat map, in order to provide a well read, distinguishable, vizualization.  

***

## b. Pallet 2

Describe the pallet and explain when you would use such a pallet.

```{r echo=FALSE, fig.align='center'}
knitr::include_graphics('1b.png')
```

***

The palette above is an example of diverging color schemes. Each scheme is
diverging because it allows the highlighting of both central and extreme values
in underlying data. Lighter shades and hues are used as the central colors in
each of these palettes. In the palettes above, each has five (5) colors with
the third (3) color being the central, lightest color. Moving away from this
color in either direction towards the first and last colors in the palette, the
shades and hues get darker. The colors diverge away from a light, neutral color
towards darker, more distinct colors. The colors at the ends of the palettes
typically contrast highly from each other to help amplify the meaning of the
divergence in the underlying data away from its central values.

Diverging colors schemes are suited to highlight the central and extreme value
in data distributions. The light coloring of central values tends to indicate
the typical values of data while the bold, contrasting coloring of extreme
values tends to highlight these extreme values. Examples could be grade
distributions, income level distributions, age ranges, etc.

Note that many datasets could be highlighted by either sequential or diverging
schemes. For example, age ranges could be highlighted by either. However, the
intent of the visualization would help dictate which to use. Consider a
question posed such as "Comparing pre-teen, teen, adult, and eldery
populations...?" Now consider a second, similar question posed such as "What are
the average ages...?" The first question is being posed from a categorial
perspective that implies a sequence tied to human lifecycles. There is implied
interest in order so a sequential color scheme would be applicable. For the
second question, there was not much emphasis on any difference between young or
old but instead more interest in the distribution, the *average*. In this case,
a diverging color scheme may be more suited to the vizualization to not only
highlight the average (central valeus) but also highlight the extremes.

***

## c. Pallet 3

Describe the pallet and explain when you would use such a pallet.

```{r echo=FALSE, fig.align='center'}
knitr::include_graphics('1c.png')
```

***

The palette above is an example of qualitative color schemes. Each scheme is
designed with a set of color shades and hues that contrast from one another.
Sequential and diverging color schemes do not try to contrast as much but
instead try to show more relationship or transitioning of values between each
color. Qualitative schemes try to show the contrast as much as possible
attempting to highlighted the grouping and differences more than the
similarities or nearness to other groups.

Qualitative color schemes are best used when tyring to depict different
categories of data that are more distinct from each other than they are as
similar or close to one another. Examples include demographic data such as
racial identity, gender identity, political affiliation, religious affiliation,
sports team fan affiliation, etc.

Some vizualization to which this color scheme could be applied, include pie charts, waffle plots, donut plots, and ring plots.  

***

\newpage
# 2) Earthquakes

```{r}
library(tidyverse)
library(ggmap)
library(WHO)
library(stringr)
library(waffle)
```



Here is the link to the USGS website where the worldwide
[earthquake](https://earthquake.usgs.gov/earthquakes/feed/v1.0/) data can be
downloaded. Download all earthquake data for the past 30 days in .csv format.
Using R, make a map of the world with points where the earthquakes occurred.
Make a bubble map using the magnitude. Thoroughly discuss your visualizations.

## World Map of Points

  As the earthquake data of the past 30 days is updated every minute, it is important to specify that the earthquake data was downloaded at 1:08 p.m., on November 18, 2019.
  
  To begin the map, I will read in the earthquake dataset under the variable name: "earthquake."
```{r}
earthquake <- read_csv("earthquake.csv", col_names = TRUE)
head(earthquake)
```

Variable | Description
---------|-----------
time | Time of Earthquake occurence
latitude | Latitude Location of Earthquake
longitude | Longitude location of Earthquake
depth | Depth of the Event
mag | Magnitude of Event
magType | Algorithm or Method Used to Evaluate the Method of the Earthquake
nst | Number of Seismic Stations used to evaluate Eathquake Location
gap | The Largest azimutahl gap between azimuthally adjacent stations (in degrees)
horizontalError | Uncertainty of Observed Event's Location (in KM)
dmin | Smallest observed Distance to event epicenter from the Closest Seismic Station
rms | Root Mean Square Calculations of Residuals in predictions of Event occurence.
net | ID of Data Contributer
id | Unique Identification of Eathquake
updated | Time of Upload in Original Dataset
place | Nearby Named Geographical Region
horizontalError | Uncertainty of Earthquake Location (in KM)
depthError | Uncertainty of Earthquake Depth (in KM)
magNst | Total number of Seismic Stations used to Calculate Earthquake's Magnitude
Status | Indicates Whether Event has been viewed by a Person
locationSource | Network that Authored location of Event
magSource | Network that Authored Preffered Magnitude
-------------------


```{r}
ggplot(data = earthquake) +
  borders("world") + 
  geom_point(mapping = aes(x = longitude, y = latitude, color = mag)) 
```

**Analysis:** As evident from the world map, it appears that the west coast of the United States has had the highest amount of recorded earthquakes in the past 30 days.  Other frequent earthquakes sites include Japan, Malaysia, Alaska, and the Phillipines.  While the West coast of the United States appears to have the biggest cluster of earthquakes, the continent and shoreline of Asia appears to have the most earthquakes with the highest magnitude.  




## Bubble Map of Earthquakes by Magnitude (With Labels)

```{r}
radius <- sqrt(earthquake$mag/ pi )
symbols(earthquake$nst, earthquake$magNst, circles = radius, inches = .35, fg = "white", bg = "salmon", xlab = "NST", ylab = "MagNST", main = "Bubble Chart with Circle Radius by Magnitude")
text(earthquake$nst, earthquake$magNst, earthquake$place, cex = .5)
```
**Analysis:**  For the Bubble Chart, we decided to utilize the variables NST, MAgNST, and the Magnitude.  The x-axis represents the amount of seismic sensors used to detect the location, and the y-axis represents the number of seismic sensors used to calculate the magnitude.  The diameter of each circle, is based off of the Magnitude for each indivudal observation. 
  As evident from the bubble chart, it appeards that there are more Location seismic sensors used than sensors used to calculate magnitude.  In addition, one can tell from the distribuion of the larger circles, that the number of seismic sensors used has no effect on evaluating the magnitude of a quake.  
  
  As the clustering of the circle's labels leads to a very distracting plot, the bubble plot has been plotted again, without the text labels. 




## Bubble Plot of Earthquakes by Magnitude (Without Labels)

```{r}
symbols(earthquake$nst, earthquake$magNst, circles = radius, inches = .35, fg = "white", bg = "salmon", xlab = "NST", ylab = "MagNST", main = "Bubble Chart with Circle Radius by Magnitude")
```




***

TO DO

***

\newpage
# 3) Disease / Illness Story

See `Final.pdf` in this project's Files section for detailed instructions.

First, I will read in the "CHOLERA" dataset from the "WHO" metadata.


```{r}
gho_vectorize <- function(v) { ### To create a clean, factored vector
  switch(class(v),
         "character" = {
           f <- as_factor(v)
           fct_explicit_na(fct_relevel(f,
                                       sort(levels(f))),
                           na_level = "NA")
         },
         "factor" = {
           fct_explicit_na(fct_relevel(v,
                                       sort(levels(v))),
                           na_level = "NA")
         },
         "numeric" = as_factor(v))
}




tb_cholera <- get_data("CHOLERA_0000000001")
tb_cholera$country <- gho_vectorize(tb_cholera$country)
tb_cholera$region <- gho_vectorize(tb_cholera$region)
tb_cholera$publishstate <- gho_vectorize(tb_cholera$publishstate)

tb_cholera <- tb_cholera %>%
  group_by(country) %>%
  arrange(country, year) %>%
  select(country, year, value, worldbankincomegroup, region) %>% rename("cases" = value)
tb_cholera
```


Next, with the dataset successfully  read in, I will now add to the "WSH_10" dataset.  The "WSH_10" dataset, records the number of deaths related to improper water sources.     My goal, is to knit these to datasets by their common variables, and gain an inference on how the cases of Cholera in a country effect the number of deaths related to contaminated water intake.  However, as the WSH-10 dataset only contains values for, 2016, I will only filter out the observations from the year 2016.  

```{r}
d_table <- get_data("WSH_10")
viz_data <- tb_cholera %>% left_join(d_table) %>% filter( year == 2016) %>% rename('deaths' = value)
viz_data
```

  As evident from the code output, the data has successfuly been read in  However, one column in particular is in need of tidying: deaths. unfortunately, this column was read in as a character vecotr.  to accomplish this, I will first remove the extra, none-numerical values.  Then, with the columns tidied up, I will now use the "dplyr" package in order to "parse" the columns into their proper forms.   
  
```{r}
viz_data$deaths <- viz_data$deaths%>% str_remove(pattern = "[:space:]\\[[:digit:]+-[:digit:]+\\]") %>% parse_integer()
viz_data$deaths

viz_data <- viz_data %>% select(-gho, -worldbankincomegroup)
viz_data
```
  
  With the dataset properly transformed, I will now utilize the package "ggplot" in order to create vizualizations of the refined dataset. 

***

**Scatterplot With Refined Smoother of Cases of Cholera by Deaths from Improper Water**
```{r}
viz_data %>% 
ggplot() + 
  geom_point(mapping = aes(x = cases, y = deaths), color = 21) +
  geom_smooth(mapping = aes(x = cases, y = deaths), method = "loess") 
```

**Analysis:** As evident from the smoothed scatterplot, it appears that as the number of cases of Cholera increases, the number of deaths from a basic water source remains relatively constant.  To me, this is very odd, as I was expecting a very positive linear trend in the dataset, suggesting a strong correlation.  However, it appears that the amount of Cholera cases is at a constant trend with the amount of deaths in a country.  

### Lollipop Plot:

```{r}
viz_data %>% select(country, cases) %>% group_by(country) %>% summarise(total_cases = sum(cases, na.rm = TRUE)) %>% 
ggplot(mapping = aes(x = country, y = total_cases)) + 
  geom_segment(mapping = aes(x = country, xend = country, y = 0, yend = total_cases)) +
  geom_point(size = 5, color = "red", fill = alpha("orange", .3), alpha = .7, shape = 21, stroke = 2) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
  ggtitle("Lollipop Plot of Cholera Cases by Country") +
  coord_flip()
```

















Next, I  will create a waffeplot for the number of Cholera Cases per country, in order to gain an inference of the distribution in each country.  To begin this process, I will first format the "viz_data" dataset into the proper format.   

```{r}
waffle_viz_data <- viz_data %>% group_by(country) %>% summarise(total_cases = sum(cases, na.rm = TRUE)) %>% pivot_wider(names_from = country, values_from = total_cases) %>% arrange(desc(total_cases)) %>% mutate(ranking = row_number())
```


Next, with the data finally formatted correctly, I will now create the waffle plot.  


```{r}
waffle(waffle_viz_data, rows = 20, colors = (c(seq(1,37))), xlab = "1 Square = 1 Observation")
```





***
